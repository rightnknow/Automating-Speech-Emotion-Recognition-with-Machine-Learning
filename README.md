# SpeechSentiments

Humans express their thoughts and emotions naturally through speech. Enabling natural interactions between humans and machines over a wide spectrum of activities from clinical monitoring to responsive entertainment systems requires automated systems capable of recognizing context and emotion in naturally occurring human speech. Thus, the next-generation human computer interfaces in these application areas will be empowered by speech-based emotional intelligence.

Classification of emotions can be achieved by analyzing the acoustical and lexical content of speech signals. Traditional methods rely heavily on features tuned to acoustical characteristics and produce robust results when combined with supervised learning algorithms. More recent works, on the other hand, are inspired by the learning capabilities of deep-learning algorithms to provide a more holistic solution.

The gap in the current state-of-the-art is a hybrid technique that takes advantages of both approaches. In this project, we will leverage existing machine learning methodologies to develop a hybrid emotion classification technique that operates both the acoustical and lexical modalities of speech. In addition, we will develop an Android front-end application to demonstrate the results of this hybrid classifier.

The system shall take in arbitrary input speech signals spoken in standard North American English and identify the six archetypal emotions: happy, sad, angry, fearful, disgust, and surprised.  Based on the assessment of the proposed design and feasibility evaluation, we aim to achieve a true positive rate of 80%.

As the project progresses, the project requirements and the proposed design will be refined. This project will conclude with the delivery of the machine learning model, an Android front-end graphical interface, a final report and a presentation near the end of the current school year.
